{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# base\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import IPython\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "#ml\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# visualization\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision.models import resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.8.1 (default, Jan  8 2020, 22:29:32) \n",
      "[GCC 7.3.0]\n",
      "SciPy version: 1.4.1\n",
      "NumPy version: 1.18.1\n",
      "scikit-learn version: 0.22.2.post1\n",
      "pandas version: 1.0.3\n",
      "matplotlib version: 3.2.1\n",
      "IPython version: 7.13.0\n",
      "OpenCV version: 4.2.0\n",
      "Torch version: 1.4.0\n",
      "Available GPUs: 1\n",
      "Torch device: cuda\n"
     ]
    }
   ],
   "source": [
    "print('Python version: {}'. format(sys.version))\n",
    "print('SciPy version: {}'. format(sp.__version__)) \n",
    "print('NumPy version: {}'. format(np.__version__))\n",
    "print('scikit-learn version: {}'. format(sklearn.__version__))\n",
    "print('pandas version: {}'. format(pd.__version__))\n",
    "print('matplotlib version: {}'. format(matplotlib.__version__))\n",
    "print('IPython version: {}'. format(IPython.__version__)) \n",
    "print('OpenCV version: {}'. format(cv2.__version__)) \n",
    "print('Torch version: {}'. format(torch.__version__))\n",
    "print('Available GPUs: {}'.format(torch.cuda.device_count()))\n",
    "if torch.cuda.is_available:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(\"Torch device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_collate(batch):\n",
    "    batch_size = len(batch)\n",
    "    images = np.array([x[0] for x in batch])\n",
    "    images = torch.from_numpy(images)\n",
    "    \n",
    "    labels = np.array([x[1] for x in batch])\n",
    "    labels = torch.from_numpy(labels)\n",
    "    labels = labels.unsqueeze(1)\n",
    "\n",
    "    assert(images.shape[0] == labels.shape[0] == batch_size)\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHESTXRAYS_PATH = Path('./input/chestxrays')\n",
    "class ChestXRaysDataset(Dataset):\n",
    "    def __init__(self, size=128, augment=None):\n",
    "        super(ChestXRaysDataset, self).__init__()\n",
    "        print('ChestXRaysDataset initialized with size={}, augment={}'.format(size, augment))\n",
    "        print('Dataset is located in {}'.format(CHESTXRAYS_PATH))\n",
    "        self.size = size\n",
    "        self.augment = augment\n",
    "        \n",
    "        train_dir = CHESTXRAYS_PATH / 'train'\n",
    "        val_dir = CHESTXRAYS_PATH / 'val'\n",
    "        test_dir = CHESTXRAYS_PATH / 'test'\n",
    "        \n",
    "        normal_cases = []\n",
    "        pneumonia_cases = []\n",
    "        for folder in [train_dir, val_dir, test_dir]:\n",
    "            normal_cases.extend((folder / 'NORMAL').glob('*.jpeg'))\n",
    "            pneumonia_cases.extend((folder / 'PNEUMONIA').glob('*.jpeg'))\n",
    "            \n",
    "        self.labels = np.concatenate((\n",
    "            np.zeros(len(normal_cases)),\n",
    "            np.ones(len(pneumonia_cases))\n",
    "        )).reshape(-1, 1)\n",
    "        images = np.concatenate((normal_cases, pneumonia_cases)).reshape(-1, 1)\n",
    "        \n",
    "        self.df = pd.DataFrame(np.concatenate((images, self.labels), axis=1), columns=['image', 'label'])\n",
    "        \n",
    "        del images\n",
    "\n",
    "        print(\"Dataset: {}\".format(self.df))\n",
    "            \n",
    "\n",
    "    @staticmethod\n",
    "    def _load_image(path, size):\n",
    "        img = Image.open(path)\n",
    "        img = cv2.resize(np.array(img), (size, size), interpolation=cv2.INTER_AREA)\n",
    "        if len(img.shape) == 2:\n",
    "            img = np.expand_dims(img, axis=2)\n",
    "            img = np.dstack([img, img, img])\n",
    "        else:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # size, size, chan -> chan, size, size\n",
    "        img = np.transpose(img, axes=[2, 0, 1])\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        img = self._load_image(row['image'], self.size)\n",
    "        label = row['label']        \n",
    "\n",
    "        if self.augment is not None:\n",
    "            img = self.augment(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID19_PATH = Path('./input/covid-chestxray-dataset')\n",
    "class COVID19Dataset(Dataset):\n",
    "    def __init__(self, size=128, augment=None):\n",
    "        super(COVID19Dataset, self).__init__()\n",
    "        print('COVID19Dataset initialized with size={}, augment={}'.format(size, augment))\n",
    "        print('Dataset is located in {}'.format(COVID19_PATH))\n",
    "        self.size = size\n",
    "        self.augment = augment\n",
    "        \n",
    "        image_dir = COVID19_PATH / 'images'\n",
    "        metadata_path = COVID19_PATH / 'metadata.csv'\n",
    "        \n",
    "        df_metadata = pd.read_csv(metadata_path, header=0)\n",
    "        # Drop CT scans\n",
    "        df_metadata = df_metadata[df_metadata.modality == 'X-ray']\n",
    "        \n",
    "        # COVID-19 = 1, SARS/ARDS/Pneumocystis/Streptococcus/No finding = 0\n",
    "        self.labels = (df_metadata.finding == 'COVID-19').values.reshape(-1, 1)\n",
    "        images = df_metadata.filename\n",
    "        images = images.apply(lambda x: image_dir / x).values.reshape(-1, 1)\n",
    "        \n",
    "        self.df = pd.DataFrame(np.concatenate((images, self.labels), axis=1), columns=['image', 'label'])\n",
    "        \n",
    "        del images\n",
    "\n",
    "        print(\"Dataset: {}\".format(self.df))\n",
    "            \n",
    "\n",
    "    @staticmethod\n",
    "    def _load_image(path, size):\n",
    "        img = Image.open(path)\n",
    "        img = cv2.resize(np.array(img), (size, size), interpolation=cv2.INTER_AREA)\n",
    "        if len(img.shape) == 2:\n",
    "            img = np.expand_dims(img, axis=2)\n",
    "            img = np.dstack([img, img, img])\n",
    "        else:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # size, size, chan -> chan, size, size\n",
    "        img = np.transpose(img, axes=[2, 0, 1])\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        img = self._load_image(row['image'], self.size)\n",
    "        label = row['label']        \n",
    "\n",
    "        if self.augment is not None:\n",
    "            img = self.augment(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=1, dropout=0.5):\n",
    "        super(Classifier, self).__init__()\n",
    "        resnet = resnet34(pretrained=True)\n",
    "        \n",
    "        self.conv1 = resnet.conv1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        self.avgpool = resnet.avgpool\n",
    "        bottleneck_features = resnet.fc.in_features\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(bottleneck_features),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(bottleneck_features, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # mean = MEAN\n",
    "        # std = STD2\n",
    "        x = x / 255.\n",
    "        # x = torch.cat([\n",
    "        #     (x[:, [0]] - mean[0]) / std[0],\n",
    "        #     (x[:, [1]] - mean[1]) / std[1],\n",
    "        #     (x[:, [2]] - mean[2]) / std[2],\n",
    "        #     (x[:, [3]] - mean[3]) / std[3],\n",
    "        # ], 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x) \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy(nn.Module):\n",
    "    def __init__(self, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "        preds = (y_pred > self.threshold).int()\n",
    "        return (preds == y_true).sum().float() / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, classifier, dataset, batch_size):\n",
    "        self.classifier = classifier\n",
    "        self.batch_size = batch_size\n",
    "        self.size = size\n",
    "        print('Trainer started with batch size: {} size: {}'.format(batch_size, size))\n",
    "\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "\n",
    "        self.train_dataset = dataset\n",
    "        self.validation_dataset = dataset\n",
    "        \n",
    "       \n",
    "        self.train_idx, self.validation_idx = train_test_split(\n",
    "            list(range(len(self.train_dataset))),\n",
    "            test_size=0.2,\n",
    "            stratify=self.train_dataset.labels\n",
    "        )\n",
    "\n",
    "        loader_params = dict(\n",
    "            batch_size=batch_size,\n",
    "            num_workers=1,\n",
    "            pin_memory=True,\n",
    "            collate_fn=null_collate\n",
    "        )\n",
    "        self.train_loader = DataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            sampler=SubsetRandomSampler(self.train_idx),\n",
    "            **loader_params\n",
    "        )\n",
    "        self.validation_loader = DataLoader(\n",
    "            dataset=self.validation_dataset,\n",
    "            sampler=SubsetRandomSampler(self.validation_idx),\n",
    "            **loader_params\n",
    "        )\n",
    "        print('Train set: {}'.format(len(self.train_idx)))\n",
    "        print('Validation set: {}'.format(len(self.validation_idx)))\n",
    "\n",
    "        self.it_per_epoch = math.ceil(len(self.train_idx) / self.batch_size)\n",
    "        print('Training with {} mini-batches per epoch'.format(self.it_per_epoch))\n",
    "\n",
    "        \n",
    "    def run(self, max_epochs=10):\n",
    "        self.classifier = self.classifier.cuda()\n",
    "        model = self.classifier\n",
    "\n",
    "        lr = 0.2\n",
    "        it = 0\n",
    "        epoch = 0\n",
    "        it_save = self.it_per_epoch * 5\n",
    "        it_log = math.ceil(self.it_per_epoch / 5)\n",
    "        it_smooth = self.it_per_epoch\n",
    "        print(\"Logging performance every {} iter, smoothing every: {} iter\".format(it_log, it_smooth))\n",
    "        \n",
    "        self.optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=0.9, weight_decay=0.0001)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, 2 * self.it_per_epoch, gamma=0.5)\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        criterion = criterion.cuda()\n",
    "        metrics = [Accuracy(), roc_auc_score]\n",
    "\n",
    "        print(\"{}'\".format(self.optimizer))\n",
    "        print(\"{}'\".format(self.scheduler))\n",
    "        print(\"{}'\".format(criterion))\n",
    "        print(\"{}'\".format(metrics))\n",
    "\n",
    "        train_loss = 0\n",
    "        train_roc = 0\n",
    "        train_acc = 0\n",
    "\n",
    "        print('                    |         VALID        |        TRAIN         |         ')\n",
    "        print(' lr     iter  epoch | loss    roc    acc   | loss    roc    acc   |  time   ')\n",
    "        print('------------------------------------------------------------------------------')\n",
    "\n",
    "        start = timer()\n",
    "        while epoch < max_epochs:\n",
    "            smoothed_train_loss = 0\n",
    "            smoothed_train_roc = 0\n",
    "            smoothed_train_acc = 0\n",
    "            smoothed_sum = 0\n",
    "\n",
    "            for inputs, labels in self.train_loader:\n",
    "                epoch = (it + 1) / self.it_per_epoch\n",
    "                \n",
    "                lr = self.scheduler.get_last_lr()[0]\n",
    "\n",
    "                # checkpoint\n",
    "                if it % it_save == 0 and it != 0:\n",
    "                    self.save(model, self.optimizer, it, epoch)\n",
    "\n",
    "                # training\n",
    "\n",
    "                model.train()\n",
    "                inputs = inputs.cuda().float()\n",
    "                labels = labels.cuda().float()\n",
    "\n",
    "                preds = model(inputs)\n",
    "                loss = criterion(preds, labels)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    batch_acc, batch_roc = [i(labels.cpu(), preds.cpu()).item() for i in metrics]\n",
    "\n",
    "                batch_loss = loss.item()\n",
    "                smoothed_train_loss += batch_loss\n",
    "                smoothed_train_roc += batch_roc\n",
    "                smoothed_train_acc += batch_acc\n",
    "                smoothed_sum += 1\n",
    "                if (it + 1) % it_smooth == 0 and it > 0:\n",
    "                    train_loss = smoothed_train_loss / smoothed_sum\n",
    "                    train_roc = smoothed_train_roc / smoothed_sum\n",
    "                    train_acc = smoothed_train_acc / smoothed_sum\n",
    "                    smoothed_train_loss = 0\n",
    "                    smoothed_train_roc = 0\n",
    "                    smoothed_train_acc = 0\n",
    "                    smoothed_sum = 0\n",
    "\n",
    "                if it % it_log == 0:\n",
    "                    print(\n",
    "                        \"{:5f} {:4d} {:5.1f} |                      | {:0.3f}  {:0.3f}  {:0.3f}  | {:6.2f}\".format(\n",
    "                            lr, it, epoch, batch_loss, batch_roc, batch_acc, timer() - start\n",
    "                        ))\n",
    "\n",
    "                it += 1\n",
    "\n",
    "            # validation\n",
    "            valid_loss, valid_m = self.do_valid(model, criterion, metrics)\n",
    "            valid_acc, valid_roc = valid_m\n",
    "\n",
    "            print(\n",
    "                \"{:5f} {:4d} {:5.1f} | {:0.3f}* {:0.3f}  {:0.3f}  | {:0.3f}* {:0.3f}  {:0.3f}  | {:6.2f}\".format(\n",
    "                    lr, it, epoch, valid_loss, valid_roc, valid_acc, train_loss, train_roc, train_acc, timer() - start\n",
    "                ))\n",
    "\n",
    "            # Data loader end\n",
    "        # Training end\n",
    "\n",
    "        self.save(model, self.optimizer, it, epoch)\n",
    "\n",
    "    def do_valid(self, model, criterion, metrics):\n",
    "        model.eval()\n",
    "        valid_num = 0\n",
    "        losses = []\n",
    "\n",
    "        for inputs, labels in self.validation_loader:\n",
    "            inputs = inputs.cuda().float()\n",
    "            labels = labels.cuda().float()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds = model(inputs)\n",
    "                loss = criterion(preds, labels)\n",
    "                m = [i(labels.cpu(), preds.cpu()).item() for i in metrics]\n",
    "\n",
    "            valid_num += len(inputs)\n",
    "            losses.append(loss.data.cpu().numpy())\n",
    "\n",
    "        assert (valid_num == len(self.validation_loader.sampler))\n",
    "        loss = np.array(losses).mean()\n",
    "        return loss, m\n",
    "    \n",
    "    def save(self, model, optimizer, iter, epoch):\n",
    "        torch.save(model.state_dict(), \"{}_model.pth\".format(iter))\n",
    "        torch.save({\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"iter\": iter,\n",
    "            \"epoch\": epoch\n",
    "        }, \"{}_optimizer.pth\".format(iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "size = 256\n",
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChestXRaysDataset(size)\n",
    "trainer = Trainer(classifier, dataset, batch_size)\n",
    "trainer.run(max_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "- http://www.cell.com/cell/fulltext/S0092-8674(18)30154-5\n",
    "- Joseph Paul Cohen, COVID-19 image data collection, https://github.com/ieee8023/covid-chestxray-dataset, 2020"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
